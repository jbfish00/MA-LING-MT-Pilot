{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a9209da8c734a24b7c71b203fbc3afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jbren\\.cache\\huggingface\\hub\\models--ai-forever--mGPT-1.3B-georgian. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9bf60ed3f740b6803df1bbbefc1b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.89M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ae4b21c1ef4118bc2313635fb6b7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716b6caf10f843489e4d7cbba62bedee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc9bd57817b4ff3aba6191e8dc31322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/582 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d863977f3c488a9a5565f71c54e8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "c:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacebdcce1b848d18edd4a03a5bd3c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/mGPT-1.3B-georgian\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ai-forever/mGPT-1.3B-georgian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Adjust filename, column names, and output file as needed\n",
    "CSV_FILE = \"conference_talks.csv\"\n",
    "GE_COL = \"GEO\"  # Name of the column holding Georgian text\n",
    "EN_COL = \"ENG\"   # Name of the column holding English text\n",
    "OUTPUT_JSON = \"parallel_conf_data.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_clean_data():\n",
    "    \"\"\"\n",
    "    Reads the CSV, drops rows that have missing or empty text\n",
    "    in either GeorgianText or EnglishText, and returns a clean DataFrame.\n",
    "    \"\"\"\n",
    "    # Load the CSV (assumes UTF-8 encoding)\n",
    "    df = pd.read_csv(CSV_FILE, encoding=\"utf-8\")\n",
    "\n",
    "    # Check for missing or NaN in either column\n",
    "    missing_df = df[df[GE_COL].isna() | df[EN_COL].isna()]\n",
    "    if not missing_df.empty:\n",
    "        print(\"Rows dropped due to missing data:\")\n",
    "        print(missing_df)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Drop those rows from the main DataFrame\n",
    "    df.dropna(subset=[GE_COL, EN_COL], inplace=True)\n",
    "\n",
    "    # Optionally, drop rows that have text but are blank (e.g., \"\")\n",
    "    blank_df = df[(df[GE_COL].str.strip() == \"\") | (df[EN_COL].str.strip() == \"\")]\n",
    "    if not blank_df.empty:\n",
    "        print(\"Rows dropped due to blank (empty) text:\")\n",
    "        print(blank_df)\n",
    "        print(\"-\" * 50)\n",
    "        df = df.drop(blank_df.index)\n",
    "\n",
    "    # Reset index to keep DataFrame tidy\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_to_translation_format(df):\n",
    "    \"\"\"\n",
    "    Converts each row to a dict with a 'translation' field\n",
    "    conforming to a typical Hugging Face seq2seq structure:\n",
    "    {\n",
    "       'translation': {\n",
    "          'src': '...',\n",
    "          'tgt': '...'\n",
    "       }\n",
    "    }\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        ge_text = row[GE_COL]\n",
    "        en_text = row[EN_COL]\n",
    "\n",
    "        # Build the dictionary in a typical huggingface 'translation' format\n",
    "        data.append({\n",
    "            \"translation\": {\n",
    "                \"ge\": ge_text,\n",
    "                \"en\": en_text\n",
    "            }\n",
    "        })\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"translation\": { \"src\": \"some text\", \"tgt\": \"some text\" }\n",
    "}\n",
    "\n",
    "def save_to_json(data_list, output_path):\n",
    "    \"\"\"\n",
    "    Saves the list of translation dictionaries to a JSON file with\n",
    "    UTF-8 encoding and no ASCII escaping (so Georgian characters remain legible).\n",
    "    \"\"\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data_list, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved parallel data to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows dropped due to missing data:\n",
      "                                                 title  \\\n",
      "188                                        უსმინე მას!   \n",
      "225  დიდი სიყვარული ჩვენი მამაზეციერის შვილების მიმართ   \n",
      "246                     მოემზადეთ ღმერთთან შესახვედრად   \n",
      "248  გამოცხადება ეკლესიისთვის, გამოცხადება ჩვენი ცხ...   \n",
      "258                                          უფლის ხმა   \n",
      "264                     არ შეგეშინდეთ სიკეთის კეთებისა   \n",
      "292                                             მამები   \n",
      "298                      სულიწმინდა თანამგზავრის როლში   \n",
      "304     „არჩეულნი იმისთვის, რომ დაამოწმონ ჩემი სახელი“   \n",
      "336                                უწყვეტი გამოცხადება   \n",
      "363                    იერემიას გოდება: ერიდეთ მონობას   \n",
      "372                 მორჩილებას ლოცვა-კურთხევები მოაქვს   \n",
      "375                                         გამოსყიდვა   \n",
      "385                                სად არის საბურველი?   \n",
      "386            სინანულისა და გადაწყვეტილებების შესახებ   \n",
      "394                შენიშნეთ დალოცვები თქვენს ცხოვებაში   \n",
      "401                                  ქრისტეს მოძღვრება   \n",
      "403                                   დასალაშქრი მთები   \n",
      "605                                        უსმინე მას!   \n",
      "643  დიდი სიყვარული ჩვენი მამაზეციერის შვილების მიმართ   \n",
      "664                     მოემზადეთ ღმერთთან შესახვედრად   \n",
      "666  გამოცხადება ეკლესიისთვის, გამოცხადება ჩვენი ცხ...   \n",
      "676                                          უფლის ხმა   \n",
      "682                     არ შეგეშინდეთ სიკეთის კეთებისა   \n",
      "710                                             მამები   \n",
      "716                      სულიწმინდა თანამგზავრის როლში   \n",
      "722     „არჩეულნი იმისთვის, რომ დაამოწმონ ჩემი სახელი“   \n",
      "754                                უწყვეტი გამოცხადება   \n",
      "781                    იერემიას გოდება: ერიდეთ მონობას   \n",
      "790                 მორჩილებას ლოცვა-კურთხევები მოაქვს   \n",
      "793                                         გამოსყიდვა   \n",
      "803                                სად არის საბურველი?   \n",
      "804            სინანულისა და გადაწყვეტილებების შესახებ   \n",
      "812                შენიშნეთ დალოცვები თქვენს ცხოვებაში   \n",
      "819                                  ქრისტეს მოძღვრება   \n",
      "821                                   დასალაშქრი მთები   \n",
      "\n",
      "                              speaker  \\\n",
      "188        პრეზიდენტ რასელ მ. ნელსონი   \n",
      "225           უხუცესი ქუენტინ ლ. კუკი   \n",
      "246           უხუცესი ქუენტინ ლ. კუკი   \n",
      "248  პრეზიდენტ რასელ მ. ნელსონის მიერ   \n",
      "258     უხუცეს ნილ ლ. ანდერსენის მიერ   \n",
      "264  პრეზიდენტ ჰენრი ბ. აირინგის მიერ   \n",
      "292                  No Speaker Found   \n",
      "298                  No Speaker Found   \n",
      "304                  No Speaker Found   \n",
      "336                  No Speaker Found   \n",
      "363                  No Speaker Found   \n",
      "372                  No Speaker Found   \n",
      "375                  No Speaker Found   \n",
      "385                  No Speaker Found   \n",
      "386                  No Speaker Found   \n",
      "394                  No Speaker Found   \n",
      "401                  No Speaker Found   \n",
      "403                  No Speaker Found   \n",
      "605        პრეზიდენტ რასელ მ. ნელსონი   \n",
      "643           უხუცესი ქუენტინ ლ. კუკი   \n",
      "664           უხუცესი ქუენტინ ლ. კუკი   \n",
      "666  პრეზიდენტ რასელ მ. ნელსონის მიერ   \n",
      "676     უხუცეს ნილ ლ. ანდერსენის მიერ   \n",
      "682  პრეზიდენტ ჰენრი ბ. აირინგის მიერ   \n",
      "710                  No Speaker Found   \n",
      "716                  No Speaker Found   \n",
      "722                  No Speaker Found   \n",
      "754                  No Speaker Found   \n",
      "781                  No Speaker Found   \n",
      "790                  No Speaker Found   \n",
      "793                  No Speaker Found   \n",
      "803                  No Speaker Found   \n",
      "804                  No Speaker Found   \n",
      "812                  No Speaker Found   \n",
      "819                  No Speaker Found   \n",
      "821                  No Speaker Found   \n",
      "\n",
      "                                             calling  year   season  \\\n",
      "188  უკანასკნელ დღეთა წმინდანთა იესო ქრისტეს ეკლესია  2020    April   \n",
      "225                      თორმეტ მოციქულთა ქვორუმიდან  2019    April   \n",
      "246                      თორმეტ მოციქულთა ქვორუმიდან  2018    April   \n",
      "248                                 No Calling Found  2018    April   \n",
      "258                      თორმეტ მოციქულთა ქვორუმიდან  2017  October   \n",
      "264          პირველი პრეზიდენტობის პირვველი მრჩეველი  2017  October   \n",
      "292                                 No Calling Found  2016    April   \n",
      "298                                 No Calling Found  2015  October   \n",
      "304                                 No Calling Found  2015  October   \n",
      "336                                 No Calling Found  2014  October   \n",
      "363                                 No Calling Found  2013  October   \n",
      "372                                 No Calling Found  2013    April   \n",
      "375                                 No Calling Found  2013    April   \n",
      "385                                 No Calling Found  2012  October   \n",
      "386                                 No Calling Found  2012  October   \n",
      "394                                 No Calling Found  2012  October   \n",
      "401                                 No Calling Found  2012    April   \n",
      "403                                 No Calling Found  2012    April   \n",
      "605  უკანასკნელ დღეთა წმინდანთა იესო ქრისტეს ეკლესია  2020    April   \n",
      "643                      თორმეტ მოციქულთა ქვორუმიდან  2019    April   \n",
      "664                      თორმეტ მოციქულთა ქვორუმიდან  2018    April   \n",
      "666                                 No Calling Found  2018    April   \n",
      "676                      თორმეტ მოციქულთა ქვორუმიდან  2017  October   \n",
      "682          პირველი პრეზიდენტობის პირვველი მრჩეველი  2017  October   \n",
      "710                                 No Calling Found  2016    April   \n",
      "716                                 No Calling Found  2015  October   \n",
      "722                                 No Calling Found  2015  October   \n",
      "754                                 No Calling Found  2014  October   \n",
      "781                                 No Calling Found  2013  October   \n",
      "790                                 No Calling Found  2013    April   \n",
      "793                                 No Calling Found  2013    April   \n",
      "803                                 No Calling Found  2012  October   \n",
      "804                                 No Calling Found  2012  October   \n",
      "812                                 No Calling Found  2012  October   \n",
      "819                                 No Calling Found  2012    April   \n",
      "821                                 No Calling Found  2012    April   \n",
      "\n",
      "                                                   url  GEO  \\\n",
      "188  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "225  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "246  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "248  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "258  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "264  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "292  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "298  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "304  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "336  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "363  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "372  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "375  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "385  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "386  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "394  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "401  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "403  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "605  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "643  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "664  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "666  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "676  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "682  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "710  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "716  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "722  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "754  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "781  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "790  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "793  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "803  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "804  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "812  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "819  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "821  https://www.churchofjesuschrist.org/study/gene...  NaN   \n",
      "\n",
      "                                                   ENG  \n",
      "188  My dear brothers and sisters, how thankful I a...  \n",
      "225  My dear brothers and sisters, this is a unique...  \n",
      "246  Eliza R. Snow, speaking of the Kirtland Temple...  \n",
      "248  What a glorious privilege it has been to celeb...  \n",
      "258  First, a kind word for the little children. Ye...  \n",
      "264  My dear brothers and sisters, I pray humbly th...  \n",
      "292  I speak today of fathers. Fathers are fundamen...  \n",
      "298  My dear brothers and sisters, I am grateful to...  \n",
      "304  In 1996 President Gordon B. Hinckley appeared ...  \n",
      "336  My hope for us today is that we may all feel l...  \n",
      "363  Early in our marriage my wife, Mary, and I dec...  \n",
      "372  My beloved brothers and sisters, how grateful ...  \n",
      "375  In colonial times, labor was in great demand i...  \n",
      "385  In the depths of his anguish in Liberty Jail, ...  \n",
      "386  President Monson, we love you. Thank you for t...  \n",
      "394  My beloved brothers and sisters, this conferen...  \n",
      "401  Our deepest gratitude and love to Sister Beck,...  \n",
      "403  I heard President Spencer W. Kimball, in a ses...  \n",
      "605  My dear brothers and sisters, how thankful I a...  \n",
      "643  My dear brothers and sisters, this is a unique...  \n",
      "664  Eliza R. Snow, speaking of the Kirtland Temple...  \n",
      "666  What a glorious privilege it has been to celeb...  \n",
      "676  First, a kind word for the little children. Ye...  \n",
      "682  My dear brothers and sisters, I pray humbly th...  \n",
      "710  I speak today of fathers. Fathers are fundamen...  \n",
      "716  My dear brothers and sisters, I am grateful to...  \n",
      "722  In 1996 President Gordon B. Hinckley appeared ...  \n",
      "754  My hope for us today is that we may all feel l...  \n",
      "781  Early in our marriage my wife, Mary, and I dec...  \n",
      "790  My beloved brothers and sisters, how grateful ...  \n",
      "793  In colonial times, labor was in great demand i...  \n",
      "803  In the depths of his anguish in Liberty Jail, ...  \n",
      "804  President Monson, we love you. Thank you for t...  \n",
      "812  My beloved brothers and sisters, this conferen...  \n",
      "819  Our deepest gratitude and love to Sister Beck,...  \n",
      "821  I heard President Spencer W. Kimball, in a ses...  \n",
      "--------------------------------------------------\n",
      "After cleaning, 792 rows remain.\n",
      "\n",
      "Saved parallel data to parallel_conf_data.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Step 1: Load and clean CSV\n",
    "    df = load_and_clean_data()\n",
    "    print(f\"After cleaning, {len(df)} rows remain.\\n\")\n",
    "\n",
    "    # Step 2: Convert to translation format\n",
    "    parallel_conf_data = convert_df_to_translation_format(df)\n",
    "\n",
    "    # Step 3: Save to JSON\n",
    "    save_to_json(parallel_conf_data, OUTPUT_JSON)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_and_prepare_data(json_file):\n",
    "    \"\"\"\n",
    "    Loads parallel data in JSON, converts each record\n",
    "    into a prompt-completion pair suitable for causal LM.\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Each item has item[\"translation\"][\"en\"] and item[\"translation\"][\"ge\"]\n",
    "    # We'll create a list of dicts with fields 'prompt' and 'completion'\n",
    "    records = []\n",
    "    for item in data:\n",
    "        en_text = item[\"translation\"][\"en\"]\n",
    "        ge_text = item[\"translation\"][\"ge\"]\n",
    "\n",
    "        # Construct the prompt\n",
    "        prompt = (f\"Translate from English to Georgian:\\n\"\n",
    "                  f\"English: {en_text}\\n\"\n",
    "                  f\"Georgian:\")\n",
    "\n",
    "        # The model should continue with the correct Georgian text\n",
    "        completion = f\" {ge_text}\"\n",
    "\n",
    "        records.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"completion\": completion\n",
    "        })\n",
    "\n",
    "    # Shuffle if desired\n",
    "    random.shuffle(records)\n",
    "\n",
    "    # Convert to a huggingface Dataset\n",
    "    return Dataset.from_list(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import Unigram\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import UnigramTrainer\n",
    "\n",
    "def train_unigram_tokenizer(json_file_path, vocab_size=32000, limit_alphabet=6000):\n",
    "    \"\"\"\n",
    "    Trains a custom Unigram SentencePiece tokenizer using the tokenizers library.\n",
    "    Returns a trained `tokenizers.Tokenizer` object.\n",
    "    \"\"\"\n",
    "    # 1) Initialize a tokenizer with the Unigram model\n",
    "    tokenizer = Tokenizer(Unigram())\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # 2) Create a trainer for the Unigram model\n",
    "    trainer = UnigramTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        show_progress=True,\n",
    "        special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\", \"<mask>\"],\n",
    "        # limit_alphabet can help control how many unique characters are included\n",
    "        limit_alphabet=limit_alphabet\n",
    "    )\n",
    "\n",
    "    # 3) Train on your corpus\n",
    "    tokenizer.train(\n",
    "        files=[json_file_path],\n",
    "        trainer=trainer\n",
    "    )\n",
    "\n",
    "    # 4) Set the tokenizer's post-processing or normalizer if needed\n",
    "    # (For example, you could configure a BertNormalizer or NFD normalization.)\n",
    "\n",
    "    return tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "def create_fast_tokenizer(tokenizer, model_name=\"custom-sentencepiece-unigram\"):\n",
    "    \"\"\"\n",
    "    Wraps a `tokenizers.Tokenizer` (Unigram SentencePiece) into a\n",
    "    Hugging Face PreTrainedTokenizerFast for downstream usage.\n",
    "    \"\"\"\n",
    "    fast_tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        # Provide additional arguments to help HF understand special tokens\n",
    "        # or naming conventions\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\"\n",
    "    )\n",
    "    fast_tokenizer.name_or_path = model_name\n",
    "    return fast_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e611b87a214726aaa0a2fc925697c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Translate: English: Hello world\\nGeorgian:', 'completion': ' გამარჯობა მსოფლიო', 'input_ids': [8675, 2615, 30, 8108, 30, 74, 2796, 51, 179, 24297, 124, 30, 23390, 1854, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [8675, 2615, 30, 8108, 30, 74, 2796, 51, 179, 24297, 124, 30, 23390, 1854, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def main():\n",
    "    # 1) Train or load existing tokenizer\n",
    "    if not os.path.exists(\"my_unigram_tokenizer.json\"):\n",
    "        # Train a new tokenizer\n",
    "        tokenizer_sp = train_unigram_tokenizer(\"parallel_conf_data.json\", vocab_size=32000)\n",
    "        # Save the raw tokenizer object\n",
    "        tokenizer_sp.save(\"my_unigram_tokenizer.json\")\n",
    "    else:\n",
    "        # Load existing tokenizer from file\n",
    "        from tokenizers import Tokenizer\n",
    "        tokenizer_sp = Tokenizer.from_file(\"my_unigram_tokenizer.json\")\n",
    "\n",
    "    # 2) Wrap in a PreTrainedTokenizerFast\n",
    "    fast_tokenizer = create_fast_tokenizer(tokenizer_sp, model_name=\"my-unigram-tokenizer\")\n",
    "\n",
    "    # Optionally, save so you can load it like any HF tokenizer\n",
    "    fast_tokenizer.save_pretrained(\"my_unigram_tokenizer\")\n",
    "\n",
    "    # 3) Use it in a standard Hugging Face pipeline, e.g. training your model\n",
    "    # Example: We'll create a trivial dataset to illustrate a tokenize_function\n",
    "    from datasets import Dataset\n",
    "\n",
    "    # Suppose we want each row to have 'prompt' + 'completion' for a causal LM\n",
    "    data = [\n",
    "        {\"prompt\": \"Translate: English: Hello world\\nGeorgian:\", \"completion\": \" გამარჯობა მსოფლიო\"},\n",
    "        {\"prompt\": \"Translate: English: How are you?\\nGeorgian:\", \"completion\": \" როგორ ხარ?\"},\n",
    "    ]\n",
    "    ds = Dataset.from_list(data)\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # We'll combine the prompt and completion\n",
    "        joined = [p + c for p, c in zip(examples[\"prompt\"], examples[\"completion\"])]\n",
    "        out = fast_tokenizer(joined, padding=\"max_length\", truncation=True, max_length=128)\n",
    "        out[\"labels\"] = out[\"input_ids\"].copy()  # causal LM style\n",
    "        return out\n",
    "\n",
    "    tokenized_ds = ds.map(tokenize_function, batched=True)\n",
    "\n",
    "    # At this point, tokenized_ds is ready for a Hugging Face Trainer\n",
    "    # using any model (like ai-forever/mGPT-1.3B-georgian).\n",
    "    print(tokenized_ds[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, Trainer, TrainingArguments\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai-forever/mGPT-1.3B-georgian\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy_mgpt_ft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# if your GPU supports float16\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     18\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_ds,  \u001b[38;5;66;03m# from the snippet above\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# If you want dynamic padding or something, see DataCollatorForLanguageModeling\u001b[39;00m\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m<string>:114\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\training_args.py:1400\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(version\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mbase_version) \u001b[38;5;241m==\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[0;32m   1395\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1398\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1399\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m-> 1400\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1401\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1402\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (get_xla_device_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1404\u001b[0m ):\n\u001b[0;32m   1405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or NPU devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1408\u001b[0m     )\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1412\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1418\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\training_args.py:1857\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1854\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   1855\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1856\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 1857\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\utils\\generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     52\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\training_args.py:1772\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available(min_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.20.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1772\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   1773\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1774\u001b[0m         )\n\u001b[0;32m   1775\u001b[0m     AcceleratorState\u001b[38;5;241m.\u001b[39m_reset_state(reset_partial_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ai-forever/mGPT-1.3B-georgian\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_mgpt_ft\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True  # if your GPU supports float16\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds,  # from the snippet above\n",
    "    data_collator=None, # If you want dynamic padding or something, see DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat486",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
