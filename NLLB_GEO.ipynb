{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET UNIVERSAL ENVIRONMENT VARIABLES\n",
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "# According to the NLLB docs, English is \"eng_Latn\" and Georgian is \"kat_Geor\".\n",
    "SRC_LANG = \"eng_Latn\"\n",
    "TGT_LANG = \"kat_Geor\"\n",
    "\n",
    "DATA_FILE = \"train_conf.json\"  # Your parallel data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the data\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the parallel data from a JSON file\n",
    "# The JSON file should contain a list of dictionaries with \"source\" and \"target\" keys.\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Read the entire JSON into a list of dicts\n",
    "with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Convert to a huggingface Dataset\n",
    "dataset = Dataset.from_list(raw_data)\n",
    "\n",
    "# Let's do a train/test split: 90% train, 10% test\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Preprocess the data\n",
    "# We need to tokenize the input and target texts.\n",
    "# The tokenizer needs to know the source and target languages.\n",
    "# The tokenizer will automatically add the language codes to the input text.\n",
    "# The tokenizer will also add the language codes to the target text.\n",
    "def preprocess_function(examples):\n",
    "    # We’ll store the “en” text in source, “ka” text in target\n",
    "    src_texts = []\n",
    "    tgt_texts = []\n",
    "\n",
    "    for item in examples[\"translation\"]:\n",
    "        en_text = item[\"en\"]\n",
    "        ka_text = item[\"ge\"]\n",
    "\n",
    "        # For NLLB, set the correct source/target language code:\n",
    "        # e.g. we want to translate from English -> Georgian:\n",
    "        #   src_lang = \"eng_Latn\"\n",
    "        #   tgt_lang = \"kat_Geor\"\n",
    "        src_texts.append(en_text)\n",
    "        tgt_texts.append(ka_text)\n",
    "\n",
    "    # The tokenizer must know we are dealing with these language codes\n",
    "    # for input and output\n",
    "    tokenizer.src_lang = SRC_LANG\n",
    "    tokenizer.tgt_lang = TGT_LANG\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        src_texts,\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # For seq2seq, we also prepare labels\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            tgt_texts,\n",
    "            max_length=256,\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df36c2136e54528bf50ab6940ff4de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/640 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1674d6e2689d4ca1b905d8f4e1ae9f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/72 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Tokenize the dataset\n",
    "# We can use the map function to apply the preprocessing function to the dataset.\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"translation\"]  # remove original text to keep dataset clean\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"translation\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Created 21208 line-based pairs in 'train_conf_line.json'.\n"
     ]
    }
   ],
   "source": [
    "###Separation and Algignemtn of talks by newline character\n",
    "### This script will take the original JSON file and split the text by newlines, aligning them properly. \n",
    "### This is chosen instead of by sentence since there is not perfect algiangment between en and ge sentences\n",
    "\n",
    "import json\n",
    "\n",
    "def split_by_newline(text):\n",
    "    \"\"\"\n",
    "    Splits the given text by newlines.\n",
    "    Strips each line and discards empty lines.\n",
    "    \"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    # Clean up any trailing spaces, remove empty lines\n",
    "    lines = [l.strip() for l in lines if l.strip()]\n",
    "    return lines\n",
    "\n",
    "def main():\n",
    "    input_json_path = \"train_conf.json\"        # The input JSON array file\n",
    "    output_json_path = \"train_conf_line.json\"  # Output\n",
    "\n",
    "    # 1) Load the JSON array\n",
    "    with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        # data is a list of objects, each something like:\n",
    "        # {\n",
    "        #   \"translation\": { \"ge\": \"...\", \"en\": \"...\" }\n",
    "        # }\n",
    "\n",
    "    all_pairs = []\n",
    "\n",
    "    # 2) For each item in the array, split by newlines\n",
    "    for item_idx, item in enumerate(data):\n",
    "        translation = item[\"translation\"]\n",
    "        ge_text = translation[\"ge\"]\n",
    "        en_text = translation[\"en\"]\n",
    "\n",
    "        # 3) Split each text into lines\n",
    "        ge_lines = split_by_newline(ge_text)\n",
    "        en_lines = split_by_newline(en_text)\n",
    "\n",
    "        # 4) Pair them up\n",
    "        min_len = min(len(ge_lines), len(en_lines))\n",
    "        for i in range(min_len):\n",
    "            all_pairs.append({\n",
    "                \"translation\": {\n",
    "                    \"ge\": ge_lines[i],\n",
    "                    \"en\": en_lines[i]\n",
    "                }\n",
    "            })\n",
    "\n",
    "    # 5) Save the new array of line-based pairs\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(all_pairs, f_out, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Done! Created {len(all_pairs)} line-based pairs in '{output_json_path}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Created 2497 line-based pairs in 'test_conf_line.json'.\n"
     ]
    }
   ],
   "source": [
    "###Separation and Algignemtn of talks by newline character\n",
    "### This script will take the original JSON file and split the text by newlines, aligning them properly. \n",
    "### This is chosen instead of by sentence since there is not perfect algiangment between en and ge sentences\n",
    "\n",
    "import json\n",
    "\n",
    "def split_by_newline(text):\n",
    "    \"\"\"\n",
    "    Splits the given text by newlines.\n",
    "    Strips each line and discards empty lines.\n",
    "    \"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    # Clean up any trailing spaces, remove empty lines\n",
    "    lines = [l.strip() for l in lines if l.strip()]\n",
    "    return lines\n",
    "\n",
    "def main():\n",
    "    input_json_path = \"test_conf.json\"        # The input JSON array file\n",
    "    output_json_path = \"test_conf_line.json\"  # Output\n",
    "\n",
    "    # 1) Load the JSON array\n",
    "    with open(input_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "        # data is a list of objects, each something like:\n",
    "        # {\n",
    "        #   \"translation\": { \"ge\": \"...\", \"en\": \"...\" }\n",
    "        # }\n",
    "\n",
    "    all_pairs = []\n",
    "\n",
    "    # 2) For each item in the array, split by newlines\n",
    "    for item_idx, item in enumerate(data):\n",
    "        translation = item[\"translation\"]\n",
    "        ge_text = translation[\"ge\"]\n",
    "        en_text = translation[\"en\"]\n",
    "\n",
    "        # 3) Split each text into lines\n",
    "        ge_lines = split_by_newline(ge_text)\n",
    "        en_lines = split_by_newline(en_text)\n",
    "\n",
    "        # 4) Pair them up\n",
    "        min_len = min(len(ge_lines), len(en_lines))\n",
    "        for i in range(min_len):\n",
    "            all_pairs.append({\n",
    "                \"translation\": {\n",
    "                    \"ge\": ge_lines[i],\n",
    "                    \"en\": en_lines[i]\n",
    "                }\n",
    "            })\n",
    "\n",
    "    # 5) Save the new array of line-based pairs\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(all_pairs, f_out, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Done! Created {len(all_pairs)} line-based pairs in '{output_json_path}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import stanza\n",
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# def load_single_json(json_path):\n",
    "#     \"\"\"\n",
    "#     If the JSON is a list with at least one object like:\n",
    "#       [\n",
    "#         {\n",
    "#           \"translation\": {\n",
    "#             \"ge\": \"...\",\n",
    "#             \"en\": \"...\"\n",
    "#           }\n",
    "#         }\n",
    "#       ]\n",
    "#     Returns ge_text, en_text from the first item.\n",
    "#     If multiple items exist, adapt as needed.\n",
    "#     \"\"\"\n",
    "#     with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         data = json.load(f)  # data is a list\n",
    "#     # pick first element\n",
    "#     item = data[0]\n",
    "#     translation = item[\"translation\"]\n",
    "#     ge_text = translation[\"ge\"]\n",
    "#     en_text = translation[\"en\"]\n",
    "#     return ge_text, en_text\n",
    "\n",
    "# def stanza_sentence_split(text, lang=\"ka\"):\n",
    "#     \"\"\"\n",
    "#     Use Stanza to do sentence segmentation in the given language (lang).\n",
    "#     Returns a list of sentence strings.\n",
    "#     Make sure you've run (in Python) e.g.:\n",
    "#       stanza.download('ka')\n",
    "#       stanza.download('en')\n",
    "#     before calling this function for the first time.\n",
    "#     \"\"\"\n",
    "#     nlp = stanza.Pipeline(lang=lang, processors='tokenize', use_gpu=False)\n",
    "#     doc = nlp(text)\n",
    "#     sentences = []\n",
    "#     for sent in doc.sentences:\n",
    "#         sentences.append(sent.text.strip())\n",
    "#     return sentences\n",
    "\n",
    "# def main():\n",
    "#     input_json = \"train_conf.json\"       # The input JSON array with \"ge\"/\"en\" text\n",
    "#     output_json = \"train_conf_sentence.json\"\n",
    "\n",
    "#     # 1) Load your big text from the first item in the JSON\n",
    "#     ge_text, en_text = load_single_json(input_json)\n",
    "\n",
    "#     # 2) Sentence-split with Stanza\n",
    "#     ge_sentences = stanza_sentence_split(ge_text, lang=\"ka\")\n",
    "#     en_sentences = stanza_sentence_split(en_text, lang=\"en\")\n",
    "\n",
    "#     # 3) Write the splitted lines to temp files for HunAlign\n",
    "#     ge_tempfile = \"temp_ge.txt\"\n",
    "#     en_tempfile = \"temp_en.txt\"\n",
    "#     with open(ge_tempfile, \"w\", encoding=\"utf-8\") as gf:\n",
    "#         for s in ge_sentences:\n",
    "#             gf.write(s + \"\\n\")\n",
    "\n",
    "#     with open(en_tempfile, \"w\", encoding=\"utf-8\") as ef:\n",
    "#         for s in en_sentences:\n",
    "#             ef.write(s + \"\\n\")\n",
    "#     new_array = []  # This will hold the final aligned sentence pairs\n",
    "\n",
    "#     # 4) Call HunAlign\n",
    "#     hunalign_bin = \"/path/to/hunalign\"  # or just \"hunalign.exe\" if in PATH\n",
    "#     dictionary = \"empty.dic\"           # or a real dictionary file\n",
    "#     cmd = [hunalign_bin, dictionary, ge_tempfile, en_tempfile, \"-text\"]\n",
    "\n",
    "#     result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "#     aligned_output = result.stdout\n",
    "\n",
    "#     # 5) Parse hunalign output into a final JSON array\n",
    "#     # Format: ge_sentence \\t en_sentence \\t alignment_score\n",
    "#     new_array = []\n",
    "#     for line in aligned_output.split(\"\\n\"):\n",
    "#         line = line.strip()\n",
    "#         if not line:\n",
    "#             continue\n",
    "#         parts = line.split(\"\\t\")\n",
    "#         if len(parts) < 2:\n",
    "#             continue\n",
    "#         ge_sent = parts[0].strip()\n",
    "#         en_sent = parts[1].strip()\n",
    "#         new_array.append({\n",
    "#             \"translation\": {\n",
    "#                 \"ge\": ge_sent,\n",
    "#                 \"en\": en_sent\n",
    "#             }\n",
    "#         })\n",
    "\n",
    "#     # 6) Save the aligned sentences\n",
    "#     with open(output_json, \"w\", encoding=\"utf-8\") as out_f:\n",
    "#         json.dump(new_array, out_f, ensure_ascii=False, indent=2)\n",
    "\n",
    "#     # optional cleanup\n",
    "#     try:\n",
    "#         os.remove(ge_tempfile)\n",
    "#         os.remove(en_tempfile)\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "#     print(f\"Created {len(new_array)} aligned sentence pairs in {output_json}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbren\\AppData\\Local\\Temp\\ipykernel_15916\\3106191055.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='960' max='960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [960/960 4:21:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.737200</td>\n",
       "      <td>1.416623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.542800</td>\n",
       "      <td>1.332442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.448800</td>\n",
       "      <td>1.307248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=960, training_loss=1.6152222047249476, metrics={'train_runtime': 15676.1389, 'train_samples_per_second': 0.122, 'train_steps_per_second': 0.061, 'total_flos': 1040210209013760.0, 'train_loss': 1.6152222047249476, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Training the model\n",
    "# We will use the Trainer API from Hugging Face to train the model.\n",
    "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "\n",
    "# A DataCollator will pad dynamically at batch time\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"nllb_finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,  # keep only the last checkpoint\n",
    "    fp16=True,  # if your GPU supports it\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 01:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3072477579116821, 'eval_runtime': 67.4299, 'eval_samples_per_second': 1.068, 'eval_steps_per_second': 0.534, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('nllb_finetuned\\\\tokenizer_config.json',\n",
       " 'nllb_finetuned\\\\special_tokens_map.json',\n",
       " 'nllb_finetuned\\\\sentencepiece.bpe.model',\n",
       " 'nllb_finetuned\\\\added_tokens.json',\n",
       " 'nllb_finetuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Evaluate the model\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "trainer.save_model(\"nllb_finetuned\")\n",
    "tokenizer.save_pretrained(\"nllb_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "და გვპირდება: \"ჩვენ აღვთანხმდით ღმერთთან, რომ ის არ დაიღალებს ჩვენთან და ჩვენ არ დავმსრულებთ მის დიდებას ჩვენთან ერთად\".\n"
     ]
    }
   ],
   "source": [
    "#### Inference\n",
    "# Now we can use the fine-tuned model for inference.\n",
    "from transformers import pipeline\n",
    "\n",
    "# Reload your model\n",
    "model_ft = AutoModelForSeq2SeqLM.from_pretrained(\"nllb_finetuned\")\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(\"nllb_finetuned\")\n",
    "\n",
    "# Create a pipeline for translation\n",
    "translator = pipeline(\n",
    "    \"translation\",\n",
    "    model=model_ft,\n",
    "    tokenizer=tokenizer_ft,\n",
    "    src_lang=\"eng_Latn\",\n",
    "    tgt_lang=\"kat_Geor\",\n",
    ")\n",
    "\n",
    "# If needed, set the correct language codes again:\n",
    "tokenizer_ft.src_lang = \"en\"\n",
    "tokenizer_ft.tgt_lang = \"ka\"\n",
    "\n",
    "test_en = \"We have been promised, “Because of our covenant with God, He will never tire in His efforts to help us, and we will never exhaust His merciful patience with us.”\"\n",
    "result = translator(test_en, max_length=2500)\n",
    "print(result[0][\"translation_text\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "და გვპირდებოდა: \"ჩვენთვის აღთქმა იყო, რომ ღმერთი არ დაიღალება თავის ძალისხმევაში და ჩვენთან ერთად ვერც კი ამოწურავთ მისი მოთმინებას\".\n"
     ]
    }
   ],
   "source": [
    "## If you want to use the original model for inference, you can do so as well.\n",
    "# Load the original model again (not fine-tuned)\n",
    "model_ft = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "\n",
    "\n",
    "\n",
    "translator = pipeline(\n",
    "    \"translation\",\n",
    "    model=model_ft,\n",
    "    tokenizer=tokenizer_ft,\n",
    "    src_lang=\"eng_Latn\",\n",
    "    tgt_lang=\"kat_Geor\",\n",
    ")\n",
    "\n",
    "# If needed, set the correct language codes again:\n",
    "tokenizer_ft.src_lang = \"en\"\n",
    "tokenizer_ft.tgt_lang = \"ka\"\n",
    "\n",
    "test_en = \"We have been promised, “Because of our covenant with God, He will never tire in His efforts to help us, and we will never exhaust His merciful patience with us.”\"\n",
    "result = translator(test_en, max_length=2500)\n",
    "print(result[0][\"translation_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 85 test lines from test_conf_line_copy.json\n",
      "Loading model/tokenizer from nllb_finetuned...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 124\u001b[0m\n\u001b[0;32m    121\u001b[0m     run_translation_main_finetuned()   \n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 124\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 121\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03mExample single entry point. \u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03mYou can pick which main function to run or parse arguments in a real scenario.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m#run_translation_main_baseline()\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m \u001b[43mrun_translation_main_finetuned\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 109\u001b[0m, in \u001b[0;36mrun_translation_main_finetuned\u001b[1;34m()\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(en_texts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m test lines from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_json\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# 2) Generate translations from fine-tuned model\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m finetuned_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_translations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinetuned_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43men_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetuned_outputs.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_f:\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m finetuned_outputs:\n",
      "Cell \u001b[1;32mIn[7], line 56\u001b[0m, in \u001b[0;36mgenerate_translations\u001b[1;34m(model_name_or_path, src_texts, max_length, num_beams)\u001b[0m\n\u001b[0;32m     54\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39mmax_length, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Generate\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m gen_tokens \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m     58\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[0;32m     59\u001b[0m     num_beams\u001b[38;5;241m=\u001b[39mnum_beams,\n\u001b[0;32m     60\u001b[0m     early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     61\u001b[0m )\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m     63\u001b[0m translated \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(gen_tokens, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\generation\\utils.py:2345\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[0;32m   2338\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2339\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2340\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2341\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2342\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2343\u001b[0m     )\n\u001b[0;32m   2344\u001b[0m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2345\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[0;32m   2346\u001b[0m         input_ids,\n\u001b[0;32m   2347\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2348\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2349\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2350\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2351\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2352\u001b[0m     )\n\u001b[0;32m   2354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2355\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2356\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2357\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2358\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2364\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2365\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\generation\\utils.py:3760\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3757\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m   3758\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m-> 3760\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3763\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3764\u001b[0m     model_outputs,\n\u001b[0;32m   3765\u001b[0m     model_kwargs,\n\u001b[0;32m   3766\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3767\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:1581\u001b[0m, in \u001b[0;36mM2M100ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1576\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1577\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1578\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1579\u001b[0m         )\n\u001b[1;32m-> 1581\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1592\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1598\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1600\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:1486\u001b[0m, in \u001b[0;36mM2M100Model.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1479\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1480\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1481\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1482\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1483\u001b[0m     )\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1486\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1495\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:1344\u001b[0m, in \u001b[0;36mM2M100Decoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1331\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m   1332\u001b[0m             decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m   1333\u001b[0m             hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1341\u001b[0m             use_cache,\n\u001b[0;32m   1342\u001b[0m         )\n\u001b[0;32m   1343\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1344\u001b[0m         layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1345\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1346\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1347\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1349\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1350\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1351\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   1352\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1354\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1358\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m skip_the_layer:\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:723\u001b[0m, in \u001b[0;36mM2M100DecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    721\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;66;03m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m--> 723\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    730\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    731\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:572\u001b[0m, in \u001b[0;36mM2M100SdpaAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;66;03m# Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# partitioned across GPUs when using tensor-parallelism.\u001b[39;00m\n\u001b[0;32m    570\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)\n\u001b[1;32m--> 572\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 test lines from test_conf_line_copy.json\n",
      "Loading model/tokenizer from facebook/nllb-200-distilled-600M...\n",
      "Saved baseline outputs to baseline_outputs.txt\n",
      "Saved fine-tuned outputs to finetuned_outputs.txt\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Example single entry point. \n",
    "    You can pick which main function to run or parse arguments in a real scenario.\n",
    "    \"\"\"\n",
    "    run_translation_main_baseline()\n",
    "    #run_translation_main_finetuned()   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "CUDA available: False\n",
      "Number of CPU cores: 22\n",
      "Starting translation comparison...\n",
      "Loading JSON from test_conf_line_copy.json...\n",
      "Loaded 85 source texts and 85 reference texts in 0.00 seconds\n",
      "Sample source text: Stop it!...\n",
      "Sample reference text: შეჩერდით!...\n",
      "\n",
      "Running baseline model...\n",
      "\n",
      "Starting translations with facebook/nllb-200-distilled-600M from eng_Latn to kat_Geor...\n",
      "Loading tokenizer from facebook/nllb-200-distilled-600M...\n",
      "Tokenizer loaded\n",
      "Loading model from facebook/nllb-200-distilled-600M...\n",
      "Model loaded on cpu in 3.99 seconds\n",
      "Error: Tokenizer does not support lang_code_to_id. Attempting workaround...\n",
      "Workaround target language 'kat_Geor' ID: 256086\n",
      "Dataset size: 85 entries\n",
      "DataLoader created with batch_size=8, num_batches=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with facebook/nllb-200-distilled-600M:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 1/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 174])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with facebook/nllb-200-distilled-600M:   9%|▉         | 1/11 [01:04<10:48, 64.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 translated in 64.84 seconds\n",
      "\n",
      "Processing batch 2/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with facebook/nllb-200-distilled-600M:  18%|█▊        | 2/11 [01:35<06:43, 44.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 translated in 30.76 seconds\n",
      "\n",
      "Processing batch 3/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 63])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with facebook/nllb-200-distilled-600M:  27%|██▋       | 3/11 [01:45<03:49, 28.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 translated in 9.52 seconds\n",
      "\n",
      "Processing batch 4/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 58])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with facebook/nllb-200-distilled-600M:  36%|███▋      | 4/11 [01:53<02:23, 20.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 translated in 8.01 seconds\n",
      "\n",
      "Processing batch 5/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with facebook/nllb-200-distilled-600M:  45%|████▌     | 5/11 [03:03<03:51, 38.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 translated in 70.70 seconds\n",
      "\n",
      "Processing batch 6/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 142])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with facebook/nllb-200-distilled-600M:  55%|█████▍    | 6/11 [04:20<04:17, 51.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6 translated in 76.30 seconds\n",
      "\n",
      "Processing batch 7/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 138])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with facebook/nllb-200-distilled-600M:  64%|██████▎   | 7/11 [05:10<03:24, 51.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 translated in 50.53 seconds\n",
      "\n",
      "Processing batch 8/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with facebook/nllb-200-distilled-600M:  73%|███████▎  | 8/11 [05:44<02:17, 45.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 translated in 34.07 seconds\n",
      "\n",
      "Processing batch 9/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with facebook/nllb-200-distilled-600M:  82%|████████▏ | 9/11 [05:56<01:10, 35.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9 translated in 12.24 seconds\n",
      "\n",
      "Processing batch 10/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with facebook/nllb-200-distilled-600M:  91%|█████████ | 10/11 [06:19<00:31, 31.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 translated in 22.51 seconds\n",
      "\n",
      "Processing batch 11/11 with 5 texts\n",
      "Batch tokenized, input shape: torch.Size([5, 118])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with facebook/nllb-200-distilled-600M: 100%|██████████| 11/11 [07:00<00:00, 38.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11 translated in 40.64 seconds\n",
      "Finished facebook/nllb-200-distilled-600M in 424.15 seconds\n",
      "Baseline sample output: ჟრწ!...\n",
      "\n",
      "Running finetuned model...\n",
      "\n",
      "Starting translations with nllb_finetuned from eng_Latn to kat_Geor...\n",
      "Loading tokenizer from nllb_finetuned...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded\n",
      "Loading model from nllb_finetuned...\n",
      "Model loaded on cpu in 3.34 seconds\n",
      "Error: Tokenizer does not support lang_code_to_id. Attempting workaround...\n",
      "Workaround target language 'kat_Geor' ID: 256086\n",
      "Dataset size: 85 entries\n",
      "DataLoader created with batch_size=8, num_batches=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with nllb_finetuned:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 1/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 174])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with nllb_finetuned:   9%|▉         | 1/11 [00:44<07:27, 44.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 translated in 44.73 seconds\n",
      "\n",
      "Processing batch 2/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 100])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with nllb_finetuned:  18%|█▊        | 2/11 [01:07<04:47, 31.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2 translated in 23.04 seconds\n",
      "\n",
      "Processing batch 3/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 63])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with nllb_finetuned:  27%|██▋       | 3/11 [01:24<03:20, 25.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3 translated in 16.91 seconds\n",
      "\n",
      "Processing batch 4/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 58])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with nllb_finetuned:  36%|███▋      | 4/11 [01:41<02:31, 21.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 translated in 16.45 seconds\n",
      "\n",
      "Processing batch 5/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with nllb_finetuned:  45%|████▌     | 5/11 [01:55<01:54, 19.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5 translated in 14.56 seconds\n",
      "\n",
      "Processing batch 6/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 142])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with nllb_finetuned:  55%|█████▍    | 6/11 [02:33<02:06, 25.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6 translated in 37.43 seconds\n",
      "\n",
      "Processing batch 7/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 138])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with nllb_finetuned:  64%|██████▎   | 7/11 [03:20<02:10, 32.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7 translated in 47.57 seconds\n",
      "\n",
      "Processing batch 8/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 172])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with nllb_finetuned:  73%|███████▎  | 8/11 [04:19<02:02, 40.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8 translated in 58.45 seconds\n",
      "\n",
      "Processing batch 9/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 21])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with nllb_finetuned:  82%|████████▏ | 9/11 [04:24<00:59, 29.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 9 translated in 5.05 seconds\n",
      "\n",
      "Processing batch 10/11 with 8 texts\n",
      "Batch tokenized, input shape: torch.Size([8, 84])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with nllb_finetuned:  91%|█████████ | 10/11 [04:55<00:30, 30.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10 translated in 31.12 seconds\n",
      "\n",
      "Processing batch 11/11 with 5 texts\n",
      "Batch tokenized, input shape: torch.Size([5, 118])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating with nllb_finetuned: 100%|██████████| 11/11 [05:32<00:00, 30.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11 translated in 37.19 seconds\n",
      "Finished nllb_finetuned in 335.86 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned sample output: ჟრწ!...\n",
      "\n",
      "Writing results to translation_comparison.tsv...\n",
      "Results saved successfully\n",
      "\n",
      "Calculating BLEU scores...\n",
      "Baseline BLEU: 8.88\n",
      "Finetuned BLEU: 11.87\n",
      "\n",
      "Total runtime: 760.55 seconds\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sacrebleu\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_num_threads(os.cpu_count())\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Number of CPU cores: {os.cpu_count()}\")\n",
    "\n",
    "# Define a custom Dataset class to handle source texts for translation\n",
    "class TranslationDataset(Dataset):\n",
    "    # Initialize the dataset with a list of source texts\n",
    "    def __init__(self, src_texts):\n",
    "        self.src_texts = src_texts  # Store the source texts (e.g., English sentences)\n",
    "    \n",
    "    # Return the total number of texts in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "    \n",
    "    # Retrieve a specific text by its index\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_texts[idx]  # Return the text at the given index\n",
    "\n",
    "# Function to load source and reference texts from a JSON file\n",
    "def load_json_data(json_path, src_key=\"en\", tgt_key=\"ge\"):\n",
    "    # Notify the user that JSON loading has started\n",
    "    print(f\"Loading JSON from {json_path}...\")\n",
    "    # Record the start time for performance measurement\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # Open and read the JSON file with UTF-8 encoding\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)  # Parse JSON into a Python object (list of dictionaries)\n",
    "        # Extract source texts (e.g., English) from the \"translation\" field, stripping whitespace\n",
    "        src_texts = [item[\"translation\"][src_key].strip() for item in data if \"translation\" in item and src_key in item[\"translation\"]]\n",
    "        # Extract reference texts (e.g., Georgian) from the \"translation\" field, stripping whitespace\n",
    "        ref_texts = [item[\"translation\"][tgt_key].strip() for item in data if \"translation\" in item and tgt_key in item[\"translation\"]]\n",
    "        # Report the number of loaded texts and the time taken\n",
    "        print(f\"Loaded {len(src_texts)} source texts and {len(ref_texts)} reference texts in {time.time() - start_time:.2f} seconds\")\n",
    "        # Return the lists of source and reference texts\n",
    "        return src_texts, ref_texts\n",
    "    except Exception as e:\n",
    "        # If an error occurs (e.g., file not found, invalid JSON), print it and return empty lists\n",
    "        print(f\"Error loading JSON: {e}\")\n",
    "        return [], []\n",
    "\n",
    "# Function to generate translations using a specified model\n",
    "def generate_translations(model_name_or_path, src_texts, batch_size=8, max_length=512, num_beams=1, src_lang=\"eng_Latn\", tgt_lang=\"kat_Geor\"):\n",
    "    # Announce the start of the translation process with model and language details\n",
    "    print(f\"\\nStarting translations with {model_name_or_path} from {src_lang} to {tgt_lang}...\")\n",
    "    # Record the start time for the entire translation process\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Attempt to load the tokenizer for the specified model\n",
    "    print(f\"Loading tokenizer from {model_name_or_path}...\")\n",
    "    try:\n",
    "        # Load the tokenizer with the source language specified (e.g., English as \"eng_Latn\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, src_lang=src_lang)\n",
    "        print(\"Tokenizer loaded\")  # Confirm successful loading\n",
    "    except Exception as e:\n",
    "        # If loading fails (e.g., model not found), print the error and return an empty list\n",
    "        print(f\"Error loading tokenizer: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Attempt to load the sequence-to-sequence model\n",
    "    print(f\"Loading model from {model_name_or_path}...\")\n",
    "    try:\n",
    "        # Load the model and move it to the selected device (CPU or GPU)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path).to(device)\n",
    "        # Set the model to evaluation mode (disables training-specific operations like dropout)\n",
    "        model.eval()\n",
    "        # Report successful loading with the time taken\n",
    "        print(f\"Model loaded on {device} in {time.time() - start_time:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        # If loading fails, print the error and return an empty list\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Get the token ID for the target language (e.g., \"kat_Geor\" for Georgian)\n",
    "    try:\n",
    "        # Use the tokenizer’s language code mapping to get the ID for the target language\n",
    "        tgt_lang_id = tokenizer.lang_code_to_id[tgt_lang]\n",
    "        # Display the target language and its corresponding ID\n",
    "        print(f\"Target language '{tgt_lang}' ID: {tgt_lang_id}\")\n",
    "    except AttributeError:\n",
    "        # If the tokenizer lacks lang_code_to_id (older versions), try a workaround\n",
    "        print(\"Error: Tokenizer does not support lang_code_to_id. Attempting workaround...\")\n",
    "        # Attempt to get the ID using convert_tokens_to_ids\n",
    "        tgt_lang_id = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "        # Validate the ID (NLLB language IDs are typically high, e.g., >256000)\n",
    "        if tgt_lang_id is None or tgt_lang_id < 256000:\n",
    "            print(f\"Error: Could not find valid ID for '{tgt_lang}' (got {tgt_lang_id}). Aborting.\")\n",
    "            return []\n",
    "        print(f\"Workaround target language '{tgt_lang}' ID: {tgt_lang_id}\")\n",
    "    except KeyError:\n",
    "        # If the target language isn’t in the tokenizer’s vocabulary, abort\n",
    "        print(f\"Error: '{tgt_lang}' not found in tokenizer's language codes. Aborting.\")\n",
    "        return []\n",
    "\n",
    "    # Create a dataset object from the source texts\n",
    "    dataset = TranslationDataset(src_texts)\n",
    "    # Report the number of entries in the dataset\n",
    "    print(f\"Dataset size: {len(dataset)} entries\")\n",
    "    # Create a DataLoader to batch the dataset for processing\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    # Report the batch size and total number of batches\n",
    "    print(f\"DataLoader created with batch_size={batch_size}, num_batches={len(dataloader)}\")\n",
    "\n",
    "    # Initialize an empty list to store translated outputs\n",
    "    outputs = []\n",
    "    # Disable gradient computation for faster inference\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches with a progress bar\n",
    "        for i, batch in enumerate(tqdm(dataloader, desc=f\"Translating with {model_name_or_path}\")):\n",
    "            # Record the start time for this batch\n",
    "            batch_start = time.time()\n",
    "            # Report the current batch number and size\n",
    "            print(f\"\\nProcessing batch {i+1}/{len(dataloader)} with {len(batch)} texts\")\n",
    "            \n",
    "            # Tokenize the batch of texts into tensors, padding/truncating as needed\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "            # Move the tokenized inputs to the selected device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            # Print the shape of the tokenized input (batch_size x sequence_length)\n",
    "            print(f\"Batch tokenized, input shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "            try:\n",
    "                # Generate translations using the model\n",
    "                gen_tokens = model.generate(\n",
    "                    **inputs,  # Pass tokenized inputs\n",
    "                    max_new_tokens=300,  # Limit output length to 300 new tokens\n",
    "                    num_beams=num_beams,  # Use greedy decoding (num_beams=1) for speed\n",
    "                    forced_bos_token_id=tgt_lang_id  # Force the output to start with the Georgian language token\n",
    "                )\n",
    "                # Decode the generated tokens into human-readable text, skipping special tokens\n",
    "                translated = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "                # Add the translated texts to the output list\n",
    "                outputs.extend(translated)\n",
    "                # Report the time taken for this batch\n",
    "                print(f\"Batch {i+1} translated in {time.time() - batch_start:.2f} seconds\")\n",
    "            except Exception as e:\n",
    "                # If generation fails (e.g., memory error), print the error and return partial results\n",
    "                print(f\"Error during generation: {e}\")\n",
    "                return outputs\n",
    "\n",
    "    # Report the total time taken for this model’s translations\n",
    "    print(f\"Finished {model_name_or_path} in {time.time() - start_time:.2f} seconds\")\n",
    "    # Return the list of all translated texts\n",
    "    return outputs\n",
    "\n",
    "# Main function to orchestrate the translation comparison\n",
    "def main():\n",
    "    # Announce the start of the comparison process\n",
    "    print(\"Starting translation comparison...\")\n",
    "    # Record the overall start time\n",
    "    overall_start = time.time()\n",
    "\n",
    "    # Define configuration variables\n",
    "    test_json = \"test_conf_line_copy.json\"  # Path to the JSON file with test data\n",
    "    baseline_model = \"facebook/nllb-200-distilled-600M\"  # Pretrained NLLB model\n",
    "    finetuned_model = \"nllb_finetuned\"  # Path to a finetuned model (needs replacement if valid)\n",
    "    output_file = \"translation_comparison.tsv\"  # Output file for results\n",
    "    batch_size = 8  # Number of texts to process per batch\n",
    "\n",
    "    # Load English source texts and Georgian reference texts from JSON\n",
    "    en_texts, ge_references = load_json_data(test_json)\n",
    "    # If loading fails or no texts are found, exit\n",
    "    if not en_texts or not ge_references:\n",
    "        print(\"Failed to load data. Exiting.\")\n",
    "        return\n",
    "    # Show a preview of the first source and reference texts\n",
    "    print(f\"Sample source text: {en_texts[0][:50]}...\")\n",
    "    print(f\"Sample reference text: {ge_references[0][:50]}...\")\n",
    "\n",
    "    # Run translations with the baseline model\n",
    "    print(\"\\nRunning baseline model...\")\n",
    "    baseline_outputs = generate_translations(baseline_model, en_texts, batch_size)\n",
    "    # Check if the number of outputs matches the input count\n",
    "    if len(baseline_outputs) != len(en_texts):\n",
    "        print(f\"Warning: Baseline outputs ({len(baseline_outputs)}) don’t match inputs ({len(en_texts)})\")\n",
    "    else:\n",
    "        # Show a preview of the first baseline translation\n",
    "        print(f\"Baseline sample output: {baseline_outputs[0][:50]}...\")\n",
    "\n",
    "    # Run translations with the finetuned model\n",
    "    print(\"\\nRunning finetuned model...\")\n",
    "    finetuned_outputs = generate_translations(finetuned_model, en_texts, batch_size)\n",
    "    # Check if the number of outputs matches the input count\n",
    "    if len(finetuned_outputs) != len(en_texts):\n",
    "        print(f\"Warning: Finetuned outputs ({len(finetuned_outputs)}) don’t match inputs ({len(en_texts)})\")\n",
    "    else:\n",
    "        # Show a preview of the first finetuned translation\n",
    "        print(f\"Finetuned sample output: {finetuned_outputs[0][:50]}...\")\n",
    "\n",
    "    # Write the results to a TSV file\n",
    "    print(f\"\\nWriting results to {output_file}...\")\n",
    "    try:\n",
    "        # Open the output file in write mode with UTF-8 encoding\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            # Write the header row\n",
    "            f.write(\"Source (EN)\\tBaseline (GE)\\tFinetuned (GE)\\tReference (GE)\\n\")\n",
    "            # Write each set of source, baseline, finetuned, and reference texts as a tab-separated row\n",
    "            for src, base, fine, ref in zip(en_texts, baseline_outputs, finetuned_outputs, ge_references):\n",
    "                f.write(f\"{src}\\t{base}\\t{fine}\\t{ref}\\n\")\n",
    "        print(\"Results saved successfully\")  # Confirm successful write\n",
    "    except Exception as e:\n",
    "        # If writing fails (e.g., permission error), print the error\n",
    "        print(f\"Error writing file: {e}\")\n",
    "\n",
    "    # Calculate and display BLEU scores for evaluation\n",
    "    print(\"\\nCalculating BLEU scores...\")\n",
    "    try:\n",
    "        # Compute BLEU score for baseline translations against reference texts\n",
    "        baseline_bleu = sacrebleu.corpus_bleu(baseline_outputs, [ge_references])\n",
    "        # Compute BLEU score for finetuned translations against reference texts\n",
    "        finetuned_bleu = sacrebleu.corpus_bleu(finetuned_outputs, [ge_references])\n",
    "        # Print the scores rounded to 2 decimal places\n",
    "        print(f\"Baseline BLEU: {baseline_bleu.score:.2f}\")\n",
    "        print(f\"Finetuned BLEU: {finetuned_bleu.score:.2f}\")\n",
    "    except Exception as e:\n",
    "        # If BLEU calculation fails (e.g., empty outputs), print the error\n",
    "        print(f\"Error calculating BLEU: {e}\")\n",
    "\n",
    "    # Report the total runtime for the entire process\n",
    "    print(f\"\\nTotal runtime: {time.time() - overall_start:.2f} seconds\")\n",
    "\n",
    "# Entry point: run the main function if this script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XCOMET scoring and comparison...\n",
      "Loading TSV from translation_comparison.tsv...\n",
      "Loaded 85 entries in 0.02 seconds\n",
      "\n",
      "Computing XCOMET scores with Unbabel/wmt22-comet-da...\n",
      "Downloading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38fa085cb154200b21542552a4217cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbren\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
      "Encoder model frozen.\n",
      "c:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XCOMET model loaded\n",
      "Starting prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 11/11 [00:30<00:00,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XCOMET scores computed in 40.68 seconds\n",
      "Baseline XCOMET scores written to xcomet_scores.txt\n",
      "\n",
      "Computing XCOMET scores with Unbabel/wmt22-comet-da...\n",
      "Downloading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606eeca1c1e246068adcbca3a0e74c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.5.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\jbren\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
      "Encoder model frozen.\n",
      "c:\\Users\\jbren\\AppData\\Local\\R-MINI~1\\envs\\stat486\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XCOMET model loaded\n",
      "Starting prediction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 11/11 [00:32<00:00,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XCOMET scores computed in 41.51 seconds\n",
      "Finetuned XCOMET scores written to xcomet_scores.txt\n",
      "Text files saved: src.en.txt, baseline.ge.txt, finetuned.ge.txt, ref.ge.txt\n",
      "\n",
      "Computing statistical significance...\n",
      "System 1 (Baseline): 0.7350\n",
      "System 2 (Finetuned): 0.7655\n",
      "Paired T-Test: t-statistic = -1.76, p-value = 0.082\n",
      "Bootstrap Resampling (n=300, ratio=0.4): p-value = 0.093\n",
      "Statistical significance computed in 0.03 seconds\n",
      "\n",
      "Total runtime: 82.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "def load_tsv_data(tsv_path):\n",
    "    print(f\"Loading TSV from {tsv_path}...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        df = pd.read_csv(tsv_path, sep=\"\\t\", encoding=\"utf-8\")\n",
    "        src_texts = df[\"Source (EN)\"].tolist()\n",
    "        baseline_texts = df[\"Baseline (GE)\"].tolist()\n",
    "        finetuned_texts = df[\"Finetuned (GE)\"].tolist()\n",
    "        ref_texts = df[\"Reference (GE)\"].tolist()\n",
    "        if not (len(src_texts) == len(baseline_texts) == len(finetuned_texts) == len(ref_texts)):\n",
    "            raise ValueError(\"Mismatch in number of entries across columns\")\n",
    "        print(f\"Loaded {len(src_texts)} entries in {time.time() - start_time:.2f} seconds\")\n",
    "        return src_texts, baseline_texts, finetuned_texts, ref_texts\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading TSV: {e}\")\n",
    "        return [], [], [], []\n",
    "\n",
    "def compute_xcomet_scores(src_texts, hyp_texts, ref_texts, model_path=\"Unbabel/wmt22-comet-da\"):\n",
    "    print(f\"\\nComputing XCOMET scores with {model_path}...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        print(\"Downloading model...\")\n",
    "        checkpoint_path = download_model(model_path)\n",
    "        print(\"Loading model from checkpoint...\")\n",
    "        model = load_from_checkpoint(checkpoint_path)\n",
    "        print(\"XCOMET model loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading XCOMET model: {e}\")\n",
    "        return None\n",
    "    data = [{\"src\": src, \"mt\": hyp, \"ref\": ref} for src, hyp, ref in zip(src_texts, hyp_texts, ref_texts)]\n",
    "    try:\n",
    "        print(\"Starting prediction...\")\n",
    "        scores = model.predict(data, batch_size=8, gpus=0, progress_bar=True)\n",
    "        print(f\"XCOMET scores computed in {time.time() - start_time:.2f} seconds\")\n",
    "        return scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing XCOMET scores: {e}\")\n",
    "        return None\n",
    "\n",
    "def write_scores_to_file(scores, model_name, output_file):\n",
    "    try:\n",
    "        with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\n{model_name} XCOMET Scores:\\n\")\n",
    "            f.write(f\"System-level score: {scores.system_score:.6f}\\n\")\n",
    "            avg_score = sum(scores.scores) / len(scores.scores)\n",
    "            f.write(f\"Average segment-level score: {avg_score:.6f}\\n\")\n",
    "            f.write(\"Individual segment scores:\\n\")\n",
    "            for i, score in enumerate(scores.scores):\n",
    "                f.write(f\"Sentence {i+1}: {score:.6f}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to file: {e}\")\n",
    "\n",
    "def save_texts_to_files(src_texts, baseline_texts, finetuned_texts, ref_texts):\n",
    "    with open(\"src.en.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(src_texts))\n",
    "    with open(\"baseline.ge.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(baseline_texts))\n",
    "    with open(\"finetuned.ge.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(finetuned_texts))\n",
    "    with open(\"ref.ge.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(ref_texts))\n",
    "    print(\"Text files saved: src.en.txt, baseline.ge.txt, finetuned.ge.txt, ref.ge.txt\")\n",
    "\n",
    "def compute_statistical_significance(baseline_scores, finetuned_scores, output_file, num_splits=300, sample_ratio=0.4):\n",
    "    \"\"\"\n",
    "    Replaces comet-compare: computes paired t-test and bootstrap resampling on XCOMET scores.\n",
    "    Args:\n",
    "        baseline_scores: XCOMET scores object for baseline system\n",
    "        finetuned_scores: XCOMET scores object for finetuned system\n",
    "        output_file: File to write results\n",
    "        num_splits: Number of bootstrap resamples (matches comet-compare default)\n",
    "        sample_ratio: Fraction of data to sample per split (matches comet-compare default)\n",
    "    \"\"\"\n",
    "    print(\"\\nComputing statistical significance...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Extract segment-level scores\n",
    "    baseline_seg_scores = np.array(baseline_scores.scores)\n",
    "    finetuned_seg_scores = np.array(finetuned_scores.scores)\n",
    "    \n",
    "    # System-level scores\n",
    "    baseline_sys_score = baseline_scores.system_score\n",
    "    finetuned_sys_score = finetuned_scores.system_score\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_value_ttest = ttest_rel(baseline_seg_scores, finetuned_seg_scores)\n",
    "    \n",
    "    # Bootstrap resampling\n",
    "    differences = finetuned_seg_scores - baseline_seg_scores\n",
    "    observed_mean_diff = np.mean(differences)\n",
    "    n_samples = int(len(differences) * sample_ratio)\n",
    "    resample_means = []\n",
    "    np.random.seed(1)  # Match comet-compare’s default seed\n",
    "    for _ in range(num_splits):\n",
    "        resample = np.random.choice(differences, size=n_samples, replace=True)\n",
    "        resample_means.append(np.mean(resample))\n",
    "    p_value_bootstrap = np.mean(np.array(resample_means) <= 0)  # One-sided: Finetuned better\n",
    "    \n",
    "    # Write results\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\nStatistical Significance Results:\\n\")\n",
    "        f.write(f\"System 1 (Baseline): {baseline_sys_score:.4f}\\n\")\n",
    "        f.write(f\"System 2 (Finetuned): {finetuned_sys_score:.4f}\\n\")\n",
    "        f.write(f\"Paired T-Test: t-statistic = {t_stat:.2f}, p-value = {p_value_ttest:.3f}\\n\")\n",
    "        f.write(f\"Bootstrap Resampling (n={num_splits}, ratio={sample_ratio}): p-value = {p_value_bootstrap:.3f}\\n\")\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"System 1 (Baseline): {baseline_sys_score:.4f}\")\n",
    "    print(f\"System 2 (Finetuned): {finetuned_sys_score:.4f}\")\n",
    "    print(f\"Paired T-Test: t-statistic = {t_stat:.2f}, p-value = {p_value_ttest:.3f}\")\n",
    "    print(f\"Bootstrap Resampling (n={num_splits}, ratio={sample_ratio}): p-value = {p_value_bootstrap:.3f}\")\n",
    "    print(f\"Statistical significance computed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "def main():\n",
    "    print(\"Starting XCOMET scoring and comparison...\")\n",
    "    overall_start = time.time()\n",
    "    tsv_path = \"translation_comparison.tsv\"\n",
    "    output_file = \"xcomet_scores.txt\"\n",
    "    \n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"XCOMET Scoring Results\\n\")\n",
    "        f.write(f\"Run started at: {time.ctime()}\\n\")\n",
    "\n",
    "    src_texts, baseline_texts, finetuned_texts, ref_texts = load_tsv_data(tsv_path)\n",
    "    if not src_texts:\n",
    "        print(\"Failed to load data. Exiting.\")\n",
    "        return\n",
    "\n",
    "    baseline_scores = compute_xcomet_scores(src_texts, baseline_texts, ref_texts)\n",
    "    if baseline_scores is not None:\n",
    "        write_scores_to_file(baseline_scores, \"Baseline\", output_file)\n",
    "        print(f\"Baseline XCOMET scores written to {output_file}\")\n",
    "\n",
    "    finetuned_scores = compute_xcomet_scores(src_texts, finetuned_texts, ref_texts)\n",
    "    if finetuned_scores is not None:\n",
    "        write_scores_to_file(finetuned_scores, \"Finetuned\", output_file)\n",
    "        print(f\"Finetuned XCOMET scores written to {output_file}\")\n",
    "\n",
    "    save_texts_to_files(src_texts, baseline_texts, finetuned_texts, ref_texts)\n",
    "    \n",
    "    # Replace run_comet_compare with our custom function\n",
    "    if baseline_scores is not None and finetuned_scores is not None:\n",
    "        compute_statistical_significance(baseline_scores, finetuned_scores, output_file)\n",
    "\n",
    "    total_time = time.time() - overall_start\n",
    "    print(f\"\\nTotal runtime: {total_time:.2f} seconds\")\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"\\nTotal runtime: {total_time:.2f} seconds\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_nllb_finetuned\\\\tokenizer_config.json',\n",
       " './my_nllb_finetuned\\\\special_tokens_map.json',\n",
       " './my_nllb_finetuned\\\\sentencepiece.bpe.model',\n",
       " './my_nllb_finetuned\\\\added_tokens.json',\n",
       " './my_nllb_finetuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##SAVE MODEL##\n",
    "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
    "\n",
    "model_to_save.save_pretrained(\n",
    "    \"./my_nllb_finetuned_split_conf\",\n",
    "    safe_serialization=True,     # will save model.safetensors\n",
    "    max_shard_size=\"2GB\"        # optional: shard to smaller files\n",
    ")\n",
    "\n",
    "tokenizer.save_pretrained(\"./my_nllb_finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5322e522184a4012ba41ae22b3ed6e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##LOAD PREVIOUSLY SAVED MODEL##\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./my_nllb_finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./my_nllb_finetuned\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat486",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
